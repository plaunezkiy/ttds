{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from math import log2\n",
    "from collections import defaultdict\n",
    "# from gensim import corpora, models\n",
    "from utils.processing import tokenize_text, process_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextAnalyser:\n",
    "    def __init__(self):\n",
    "        self.corpora = {}\n",
    "    \n",
    "    def load(self):\n",
    "        with open(\"data/collections/bible_and_quran.tsv\", \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\")\n",
    "            for row in reader:\n",
    "                corpus, verse = row\n",
    "                tokens = process_tokens(tokenize_text(verse))\n",
    "                self.corpora[corpus] = self.corpora.get(corpus, []) + [tokens]\n",
    "    \n",
    "    def calculate_MI(self):\n",
    "        \"\"\"\n",
    "        Calculate Mutual Information for each term in each corpus\n",
    "        p()\n",
    "        Returns a dictionary mapping corpus names to lists of (term, MI_score) tuples.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        corpus_tokens = {}\n",
    "        # total docs\n",
    "        N = 0\n",
    "        \n",
    "        # Count term frequencies per corpus\n",
    "        for corpus, documents in self.corpora.items():\n",
    "            N += len(documents)\n",
    "            corpus_tokens[corpus] = set()\n",
    "            \n",
    "            for doc in documents:\n",
    "                for term in doc:\n",
    "                    corpus_tokens[corpus].add(term)\n",
    "        \n",
    "        # Calculate MI for each term in each corpus\n",
    "        for corpus in self.corpora.keys():\n",
    "            results[corpus] = []\n",
    "            for term in corpus_tokens[corpus]:\n",
    "                MI = 0\n",
    "                # docs that both contain the term AND are in the corpus\n",
    "                n11 = sum([1 for doc in self.corpora[corpus] if term in doc])\n",
    "                # docs that contain the term but are not in the corpus\n",
    "                n10 = sum(\n",
    "                    [\n",
    "                        sum(\n",
    "                            [1 for doc in corpus_docs if term in doc and _corpus != corpus]\n",
    "                        ) for _corpus, corpus_docs in self.corpora.items()\n",
    "                    ]\n",
    "                )\n",
    "                # docs that don't contain the term but are in the corpus\n",
    "                n01 = sum([1 for doc in self.corpora[corpus] if term not in doc])\n",
    "                # all docs that contain the term\n",
    "                n1_ = sum(\n",
    "                    [\n",
    "                        sum(\n",
    "                            [1 for doc in corpus_docs if term in doc]\n",
    "                        ) for corpus_docs in self.corpora.values()\n",
    "                    ]\n",
    "                )\n",
    "                # all docs that are in the corpus\n",
    "                # n_1 = len(self.corpora[corpus])\n",
    "                n_1 = sum([1 for doc in self.corpora[corpus]] )\n",
    "                # docs that don't contain the term\n",
    "                n0_ = sum(\n",
    "                    sum(\n",
    "                        [1 for doc in corpus_docs if term not in doc] for corpus_docs in self.corpora.values()\n",
    "                    )\n",
    "                )\n",
    "                # docs that are not in the corpus\n",
    "                n_0 = sum(sum([1 for doc in corpus_docs if _corp != corpus] for _corp, corpus_docs in self.corpora.items()))\n",
    "                # docs that don't contain the term and are not in the corpus\n",
    "                n00 = sum(\n",
    "                    [\n",
    "                        sum(\n",
    "                            [1 for doc in corpus_docs if term not in doc and _corpus != corpus]\n",
    "                        ) for _corpus, corpus_docs in self.corpora.items()\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                # Calculate MI score\n",
    "                for a, b, c in ((n11, n1_, n_1), (n01, n0_, n_1), (n10, n1_, n_0), (n00, n0_, n_0)):\n",
    "                    try:\n",
    "                        MI += (a / N) * log2((N * a) / (b * c))\n",
    "                    except ZeroDivisionError:\n",
    "                        MI += 0\n",
    "                results[corpus].append((term, MI))\n",
    "            \n",
    "            # Sort terms by MI score in descending order\n",
    "            results[corpus] = sorted(results[corpus], key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def calculate_Chi2(self):\n",
    "        \"\"\"\n",
    "        Calculate Chi-squared for each term in each corpus\n",
    "        Returns a dictionary mapping corpus names to lists of (term, chi2_score) tuples.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        term_counts = {}\n",
    "        corpus_sizes = {}\n",
    "        total_docs = 0\n",
    "        \n",
    "        # Count term frequencies per corpus\n",
    "        for corpus, documents in self.corpora.items():\n",
    "            corpus_sizes[corpus] = len(documents)\n",
    "            total_docs += len(documents)\n",
    "            term_counts[corpus] = {}\n",
    "            \n",
    "            for doc in documents:\n",
    "                for term in doc:\n",
    "                    term_counts[corpus][term] = term_counts[corpus].get(term, 0) + 1\n",
    "        \n",
    "        # Calculate Chi-square for each term in each corpus\n",
    "        for corpus in self.corpora:\n",
    "            results[corpus] = []\n",
    "            \n",
    "            for term in term_counts[corpus]:\n",
    "                # Observed frequencies\n",
    "                O11 = term_counts[corpus].get(term, 0)  # term in this corpus\n",
    "                O12 = sum(counts.get(term, 0) for corpus_name, counts in term_counts.items() \n",
    "                        if corpus_name != corpus)  # term in other corpora\n",
    "                O21 = corpus_sizes[corpus] - O11  # other terms in this corpus\n",
    "                O22 = sum(size for name, size in corpus_sizes.items() \n",
    "                        if name != corpus) - O12  # other terms in other corpora\n",
    "                \n",
    "                N = total_docs\n",
    "                \n",
    "                # Expected frequencies\n",
    "                row1 = O11 + O12  # total term occurrences\n",
    "                row2 = O21 + O22  # total non-term occurrences\n",
    "                col1 = O11 + O21  # total corpus size\n",
    "                col2 = O12 + O22  # total other corpora size\n",
    "                \n",
    "                E11 = (row1 * col1) / N\n",
    "                E12 = (row1 * col2) / N\n",
    "                E21 = (row2 * col1) / N\n",
    "                E22 = (row2 * col2) / N\n",
    "                \n",
    "                # Calculate Chi-square\n",
    "                if E11 > 0 and E12 > 0 and E21 > 0 and E22 > 0:\n",
    "                    chi2 = (((O11 - E11) ** 2) / E11 + \n",
    "                        ((O12 - E12) ** 2) / E12 +\n",
    "                        ((O21 - E21) ** 2) / E21 + \n",
    "                        ((O22 - E22) ** 2) / E22)\n",
    "                    results[corpus].append((term, chi2))\n",
    "            \n",
    "            # Sort terms by Chi-square score in descending order\n",
    "            results[corpus] = sorted(results[corpus], key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # def run_LDA(self, k):\n",
    "    #     \"\"\"\n",
    "    #     Run LDA on the entire set of corpora with k topics.\n",
    "    #     Compute the average topic distribution for each corpus.\n",
    "    #     Returns a dictionary mapping corpus names to average topic distributions.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     # Combine all documents and track their corpus labels\n",
    "    #     all_documents = []\n",
    "    #     corpus_labels = []\n",
    "    #     for corpus, documents in self.corpora.items():\n",
    "    #         for doc in documents:\n",
    "    #             all_documents.append(doc)\n",
    "    #             corpus_labels.append(corpus)\n",
    "\n",
    "    #     # Create a dictionary and corpus for LDA\n",
    "    #     dictionary = corpora.Dictionary(all_documents)\n",
    "    #     corpus_bow = [dictionary.doc2bow(doc) for doc in all_documents]\n",
    "\n",
    "    #     # Run LDA\n",
    "    #     lda_model = models.LdaModel(corpus=corpus_bow, id2word=dictionary, num_topics=k, passes=10)\n",
    "\n",
    "    #     # Get topic distributions for each document\n",
    "    #     doc_topics = lda_model.get_document_topics(corpus_bow)\n",
    "\n",
    "    #     # Initialize per-corpus topic distributions\n",
    "    #     corpus_topic_sums = defaultdict(lambda: [0.0] * k)\n",
    "    #     corpus_doc_counts = defaultdict(int)\n",
    "\n",
    "    #     for i, topics in enumerate(doc_topics):\n",
    "    #         corpus = corpus_labels[i]\n",
    "    #         corpus_doc_counts[corpus] += 1\n",
    "    #         # Convert sparse topic distribution to dense vector\n",
    "    #         topic_dist = [0.0] * k\n",
    "    #         for topic_num, prob in topics:\n",
    "    #             topic_dist[topic_num] = prob\n",
    "    #         # Sum topic distributions\n",
    "    #         corpus_topic_sums[corpus] = [sum(x) for x in zip(corpus_topic_sums[corpus], topic_dist)]\n",
    "\n",
    "    #     # Compute average topic distributions\n",
    "    #     corpus_topic_avgs = {}\n",
    "    #     for corpus in self.corpora.keys():\n",
    "    #         doc_count = corpus_doc_counts[corpus]\n",
    "    #         if doc_count > 0:\n",
    "    #             avg_topic_dist = [value / doc_count for value in corpus_topic_sums[corpus]]\n",
    "    #             corpus_topic_avgs[corpus] = avg_topic_dist\n",
    "\n",
    "    #     return corpus_topic_avgs, lda_model\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # part 2\n",
    "    ta = TextAnalyser()\n",
    "    ta.load()\n",
    "    # Print the top 10 terms by MI score for each corpus\n",
    "    MI_scores = ta.calculate_MI()\n",
    "    # token: {col: doc}\n",
    "    print(\"MI scores:\")\n",
    "    for corpus, scores in MI_scores.items():\n",
    "        print(corpus)\n",
    "        for term, score in scores[:10]:\n",
    "            print(f\"{term}: {round(score, 3)}\")\n",
    "    # Print the top 10 terms by Chi2 score for each corpus\n",
    "    # Chi2_scores = ta.calculate_Chi2()\n",
    "    # print(\"\\nChi2 scores:\")\n",
    "    # for corpus, scores in Chi2_scores.items():\n",
    "    #     print(corpus)\n",
    "    #     for term, score in scores[:10]:\n",
    "    #         print(f\"{term}: {round(score, 3)}\")\n",
    "    k = 20  # Number of topics\n",
    "    # topic_avgs, lda_model = ta.run_LDA(k)\n",
    "    # for corpus, avg_dist in topic_avgs.items():\n",
    "    #     # Identify the topic with the highest average score\n",
    "    #     top_topic_index = avg_dist.index(max(avg_dist))\n",
    "    #     print(f\"\\n{corpus} - Top Topic {top_topic_index}:\")\n",
    "    #     # Get the top 10 tokens for this topic\n",
    "    #     top_tokens = lda_model.show_topic(top_topic_index, topn=10)\n",
    "    #     print(\"Top 10 tokens:\")\n",
    "    #     for token, prob in top_tokens:\n",
    "    #         print(f\"{token}: {round(prob, 4)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
